{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Data Loaded\n",
      "Tokenizing...\n",
      "Tokenization Completed\n",
      "Training...\n",
      "Train on 12586 samples, validate on 1399 samples\n",
      "Epoch 1/40\n",
      "12586/12586 [==============================] - 26s 2ms/step - loss: 0.6564 - acc: 0.6095 - val_loss: 0.5843 - val_acc: 0.7398\n",
      "Epoch 2/40\n",
      "12586/12586 [==============================] - 23s 2ms/step - loss: 0.5490 - acc: 0.7602 - val_loss: 0.5759 - val_acc: 0.7405\n",
      "Epoch 3/40\n",
      "12586/12586 [==============================] - 23s 2ms/step - loss: 0.5070 - acc: 0.7749 - val_loss: 0.5624 - val_acc: 0.7813\n",
      "Epoch 4/40\n",
      "12586/12586 [==============================] - 25s 2ms/step - loss: 0.4680 - acc: 0.8067 - val_loss: 0.5633 - val_acc: 0.7852\n",
      "Epoch 5/40\n",
      "12586/12586 [==============================] - 23s 2ms/step - loss: 0.4317 - acc: 0.8228 - val_loss: 0.5454 - val_acc: 0.7906\n",
      "Epoch 6/40\n",
      " 2048/12586 [===>..........................] - ETA: 19s - loss: 0.4059 - acc: 0.8250"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "from numpy import array, unique, array_equal\n",
    "import keras\n",
    "max_fatures = 3000\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading Data...\")\n",
    "    TwitterEmotion = pd.read_csv('text_emotion.csv')\n",
    "    print(\"Data Loaded\")\n",
    "    return (TwitterEmotion['content'], array(TwitterEmotion['sentiment']))\n",
    "\n",
    "\n",
    "#Below the data is sorted into nine emotion groups. Eight of the groups are the outter layer of the wheel, or the combinations of \n",
    "#two emotion groups. The ninth group is \"Ambiguous\" and \"Neutral\" put together.\n",
    "def sort_to_2_emotions(sentence_list, emotion_list):\n",
    "    sorted_list = []\n",
    "    sorted_emo = []\n",
    "    for (data, emo) in zip(sentence_list, emotion_list):\n",
    "        if (emo == 'enthusiam' or emo == 'love' or emo == 'happiness'):\n",
    "            sorted_list.append(data)\n",
    "            sorted_emo.append(\"positive\")\n",
    "        if (emo == 'sadness' or emo == 'hate'):\n",
    "            sorted_list.append(data)\n",
    "            sorted_emo.append(\"negative\")\n",
    "    return (sorted_list, sorted_emo)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "    print(\"Tokenizing...\")\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ',lower=True)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    X = tokenizer.texts_to_sequences(sentences)\n",
    "    X = pad_sequences(X)\n",
    "    print(\"Tokenization Completed\")\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def train(tokened_sentences, emotion_list):\n",
    "    print(\"Training...\")\n",
    "    embed_dim = 128\n",
    "    lstm_out = 196\n",
    "    test_percent=.1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = tokened_sentences.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(LSTM(lstm_out, dropout=0.33, recurrent_dropout=0.33))\n",
    "    \n",
    "    # len(set(emotion_list)) is a hacky way of geting the number of unique elements\n",
    "    # in a regualar python list (non-numpy)\n",
    "    model.add(Dense(unique(emotion_list).size,activation='softplus'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=True),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "\n",
    "    Y = pd.get_dummies(emotion_list).values\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(tokened_sentences,Y, test_size = test_percent, random_state = 42)\n",
    "    #print(X_train.shape,Y_train.shape)\n",
    "    #print(X_test.shape,Y_test.shape)\n",
    "\n",
    "    batch_size = 256\n",
    "    model.fit(X_train, Y_train, epochs = 40, batch_size=batch_size, verbose = 1,validation_split=.1)\n",
    "    print(\"Training Completed\")\n",
    "    print(\"Testing Against Control... (% of the data) \", test_percent)\n",
    "    score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "    print(\"Score   :\", score)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    return (model, X_test)\n",
    "\n",
    "### When Splitting data (train_test_split), we don't retain where in the \n",
    "### origional set the data is located, thus it takes\n",
    "### a little trickery to see the results while while only testing against untrained data\n",
    "### top_predictions variable changes how many predictions given\n",
    "def test(model, X_test, tokenized_data, sentence_list, emotion_list, top_predictions=1):\n",
    "    predictions = model.predict(tokenized_data, batch_size=32)\n",
    "    error = 0\n",
    "    for i in range(len(predictions)):\n",
    "     \n",
    "        #  \"If the current tokenized data array is in X_test (untrained tokenized arrays)\n",
    "        \n",
    "        if(any(array_equal(tokenized_data[i], x) for x in X_test)):\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            \n",
    "            pos = list(predictions[i]).index(max(predictions[i]))\n",
    "            if (unique(emotion_list)[pos] != emotion_list[i]):\n",
    "                error += 1\n",
    "            temp = predictions[i]\n",
    "            \n",
    "            print(sentence_list[i])\n",
    "\n",
    "            for j in range(top_predictions):\n",
    "                pos = list(temp).index(max(temp))\n",
    "                print(\"\\n # %s Predicted emotion : \",j+1, unique(emotion_list)[pos])\n",
    "                temp[pos] = 0\n",
    "            print(\"Actual emotion   : \", emotion_list[i])\n",
    "\n",
    "    print(\"%  Accuracy when against untrained set: \", 1- (float(error) / len(X_test)))\n",
    "\n",
    "#################################\n",
    "#############MAIN################\n",
    "#################################\n",
    "#   jupyter notebook is weird   #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(sentence_list, emotion_list) = load_data()\n",
    "\n",
    "\n",
    "### Comment the Below line for all 18 emotions. This sorts into \"positive\" and \"negative\"\n",
    "sentence_list, emotion_list = sort_to_2_emotions(sentence_list, emotion_list)\n",
    "\n",
    "tokenized_data = tokenize(sentence_list)\n",
    "\n",
    "(model, X_test) = train(tokenized_data, emotion_list)\n",
    "\n",
    "test(model, X_test, tokenized_data, sentence_list, emotion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import xlrd\n",
    "import nltk as nltk\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from os import path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,4))\n",
    "sns.barplot(x = ProjectData['emotion'].unique(), y=ProjectData['emotion'].value_counts())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
