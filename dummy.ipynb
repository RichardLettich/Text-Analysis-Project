{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import xlrd\n",
    "import nltk as nltk\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from os import path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Loading Data...\n",
      "Data Loaded\n",
      "negative\n",
      "Tokenizing...\n",
      "Tokenization Completed\n",
      "Existing Trained Model Found, Loading From Disk..\n",
      "\u001b[92mModel Loaded\u001b[0m\n",
      "Loading  tweets_unlabeled/out2.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7c43e6f724ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0mgraph_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmistake_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-7c43e6f724ae>\u001b[0m in \u001b[0;36mmake_comparison\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m     ]\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_unlabeled_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m        \u001b[0;31m# for k in tweets:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7c43e6f724ae>\u001b[0m in \u001b[0;36mload_unlabeled_tweets\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_blank_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mor\u001b[0m  \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mempty_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import xlrd\n",
    "import nltk as nltk\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import thinc.extra.datasets\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.style as style \n",
    "from numpy import array, unique, array_equal\n",
    "import keras\n",
    "max_fatures = 3500\n",
    "from keras import regularizers\n",
    "from scipy.stats import expon\n",
    "from matplotlib2tikz import save as tikz_save\n",
    "import os\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "def load_data_self():\n",
    "    print(\"Loading Data...\")\n",
    "    PrimaryEmotion = pd.read_csv('emotion.xls.csv')\n",
    "    print(\"Data Loaded\")\n",
    "    return (PrimaryEmotion['sentence'], array(PrimaryEmotion['emotion']))\n",
    "\n",
    "\n",
    "\n",
    "def sort_to_2_emotions_self(sentence_list, emotion_list):\n",
    "    sorted_list = []\n",
    "    sorted_emo = []\n",
    "    for (data, emo) in zip(sentence_list, emotion_list):\n",
    "        if (emo == 'Joy' or emo == 'Love' or emo == 'Optimism' or emo == 'Awe' or emo == 'Trust'):\n",
    "            sorted_list.append(data)\n",
    "            sorted_emo.append(\"positive\")\n",
    "        if (emo == 'Anger' or emo == 'Disgust' or emo == 'Sadness' or emo == 'Aggression' or emo == 'Contempt' or emo == 'Disapproval' or emo == 'Remorse'):\n",
    "            sorted_list.append(data)\n",
    "            sorted_emo.append(\"negative\")\n",
    "    return (sorted_list, sorted_emo)\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading Data...\")\n",
    "    TwitterEmotion = pd.read_csv('text_emotion.csv')\n",
    "    empty_rows = []\n",
    "\n",
    "    print(\"Data Loaded\")\n",
    "    return (TwitterEmotion['content'], TwitterEmotion['sentiment'])\n",
    "\n",
    "def load_unlabeled_tweets(filepath):\n",
    "    \n",
    "    print(\"Loading \",filepath)\n",
    "    ray = pd.read_csv(filepath,dtype=str,skip_blank_lines=True)['text']\n",
    "    for (i, j) in enumerate(ray.iterrows()):\n",
    "        if i['sentiment'] == '' or  isinstance(i['sentiment'], float):\n",
    "            empty_rows.append(j)\n",
    "    \n",
    "    TwitterEmotion.drop(ray)\n",
    "    return array()\n",
    "\n",
    "#Below the data is sorted into nine emotion groups. Eight of the groups are the outter layer of the wheel, or the combinations of \n",
    "#two emotion groups. The ninth group is \"Ambiguous\" and \"Neutral\" put together.\n",
    "def sort_to_2_emotions(sentence_list, emotion_list):\n",
    "    sorted_list = []\n",
    "    sorted_emo = []\n",
    "    for (data, emo) in zip(sentence_list, emotion_list):\n",
    "        if (emo == 'enthusiam' or emo == 'love' or emo == 'happiness'):\n",
    "            sorted_list.append(data)\n",
    "            sorted_emo.append(\"positive\")\n",
    "        if (emo == 'sadness' or emo == 'hate'):\n",
    "            sorted_list.append(data)\n",
    "            sorted_emo.append(\"negative\")\n",
    "    return (sorted_list, sorted_emo)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(sentences,return_tokenizer=False):\n",
    "    print(\"Tokenizing...\")\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ',lower=True)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    X = tokenizer.texts_to_sequences(sentences)\n",
    "    X = pad_sequences(X)\n",
    "    print(\"Tokenization Completed\")\n",
    "    if (return_tokenizer==True):\n",
    "        return(X,tokenizer)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(tokened_sentences, emotion_list,return_acc=False, force=False):\n",
    "    if((not os.path.isfile(\"model.h5\")) or (force == True) ):\n",
    "        print(bcolors.WARNING + \"No Existing Model Found, Training From Scratch\" + bcolors.ENDC)\n",
    "        embed_dim = 128\n",
    "        lstm_out = 196\n",
    "        test_percent=0\n",
    "\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_fatures, embed_dim))#input_length = tokened_sentences.shape[1]))\n",
    "        model.add(SpatialDropout1D(0.7))\n",
    "        model.add(keras.layers.Dropout(.6))\n",
    "\n",
    "        model.add(LSTM(lstm_out, dropout=0.65, recurrent_dropout=0.65))\n",
    "\n",
    "        # len(set(emotion_list)) is a hacky way of geting the number of unique elements\n",
    "        # in a regualar python list (non-numpy)\n",
    "        model.add(Dense(unique(emotion_list).size,activation='softmax'))\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=True ),metrics = ['accuracy'])\n",
    "         #print(model.summary())\n",
    "\n",
    "        Y = pd.get_dummies(emotion_list).values\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(tokened_sentences,Y, test_size = test_percent, random_state = 152321326)\n",
    "        #print(X_train.shape,Y_train.shape)\n",
    "        #print(X_test.shape,Y_test.shape)\n",
    "\n",
    "        batch_size = 16\n",
    "        hist = model.fit(X_train, Y_train, epochs = 13, batch_size=batch_size, verbose = 1,validation_split=.0)\n",
    "        print(\"Training Completed\")\n",
    "        print(\"Testing Against Control... (% of the data) \", test_percent)\n",
    "        print(\"Saving Model\")\n",
    "        model.save(\"model.h5\")\n",
    "       # score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "        #print(\"Score   :\", score)\n",
    "        #print(\"Accuracy:\", acc)\n",
    "      #  fig = plt.figure(figsize=(7,7))\n",
    "      #  plt.plot(hist.history['acc'])\n",
    "        #if(val_split != 0.0):\n",
    "      #  plt.plot(hist.history['val_acc'])\n",
    "      #  plt.title('Model Accuracy')\n",
    "      #  plt.ylabel('Accuracy')\n",
    "       # plt.xlabel('Epochs')\n",
    "        #plt.legend(['Training Set', 'Validation Set'], loc='upper left')\n",
    "      #  plt.show()\n",
    "      #  plot.savefig('Twitter_data_loss_plot.eps', format='eps', dpi=1200)\n",
    "\n",
    "       # plt.plot(hist.history['loss'])\n",
    "       # if(val_split != 0.0):\n",
    "       # plt.plot(hist.history['val_loss'])\n",
    "       # plt.title('Model Loss')\n",
    "       # plt.ylabel('loss')\n",
    "       # plt.xlabel('epoch')\n",
    "       # plt.legend(['Training Set', 'Validation Set'], loc='upper left')\n",
    "       # plt.show()\n",
    "       # plt.savefig('Twitter_data_loss_plot.eps', format='eps', dpi=1200)\n",
    "    else:\n",
    "        print(\"Existing Trained Model Found, Loading From Disk..\")\n",
    "        keras.backend.clear_session()\n",
    "        model = keras.models.load_model('model.h5')\n",
    "        print(bcolors.OKGREEN + \"Model Loaded\"+ bcolors.ENDC)\n",
    "    if(return_acc == True):\n",
    "        return (model, X_test,acc)\n",
    "    return model\n",
    "\n",
    "### When Splitting data (train_test_split), we don't retain where in the \n",
    "### origional set the data is located, thus it takes\n",
    "### a little trickery to see the results while while only testing against untrained data\n",
    "### top_predictions variable changes how many predictions given\n",
    "def test(model, X_test, tokenized_data, sentence_list, emotion_list, top_predictions=1):\n",
    "    predictions = model.predict(tokenized_data, batch_size=32)\n",
    "    error = 0\n",
    "    for i in range(len(predictions)):\n",
    "     \n",
    "        #  \"If the current tokenized data array is in X_test (untrained tokenized arrays)\n",
    "        \n",
    "        if(any(array_equal(tokenized_data[i], x) for x in X_test)):\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            \n",
    "            pos = list(predictions[i]).index(max(predictions[i]))\n",
    "            if (unique(emotion_list)[pos] != emotion_list[i]):\n",
    "                error += 1\n",
    "            temp = predictions[i]\n",
    "            \n",
    "            print(sentence_list[i])\n",
    "\n",
    "            for j in range(top_predictions):\n",
    "                pos = list(temp).index(max(temp))\n",
    "                print(\"\\n # %s Predicted emotion : \",j+1, unique(emotion_list)[pos])\n",
    "                temp[pos] = 0\n",
    "            print(\"Actual emotion   : \", emotion_list[i])\n",
    "\n",
    "    print(\"%  Accuracy when against untrained set: \", 1- (float(error) / len(X_test)))\n",
    "\n",
    "def count_errors(model, tokenized_data, sentence_list, emotion_list, X_test):\n",
    "    dims =  len(unique(emotion_list))\n",
    "    mistake_list = np.zeros((dims,dims), dtype=np.int)\n",
    "    predictions = model.predict(tokenized_data, batch_size=32)\n",
    "    print(\"Total predictions:\", len(predictions))\n",
    "    for i in range(len(predictions)):\n",
    "        if(any(array_equal(tokenized_data[i], x) for x in X_test)):\n",
    "            pos = list(predictions[i]).index(max(predictions[i]))\n",
    "            mistake_list[list(unique(emotion_list)).index(emotion_list[i])][pos] += 1\n",
    "   # for i in range(len(mistake_list)):\n",
    "   #     for j in range(len(mistake_list)):\n",
    "   #         mistake_list[i][j] = mistake_list[i][j] * 100 / list(emotion_list).count(unique(emotion_list)[i]) \n",
    "    print(mistake_list)\n",
    "    return mistake_list\n",
    "\n",
    "\n",
    "def graph_errors(mistake_list, emotion_list):\n",
    "    dim = len(mistake_list[0])\n",
    "    \n",
    "    false =  np.zeros((dim,4), dtype=np.int)\n",
    "    \n",
    "    #total number of testing data for each category\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            false[i][0] += mistake_list[i][j]\n",
    "\n",
    "    \n",
    "    #true positives\n",
    "    for i in range(dim):\n",
    "              false[i][1] = mistake_list[i][i]\n",
    "    #false positives\n",
    "    for i in range(dim):\n",
    "        sum = 0\n",
    "        for j in range(dim):\n",
    "            if (i != j):\n",
    "                sum += mistake_list[i][j]\n",
    "        false[i][2] = sum\n",
    "   #     \n",
    "  #          #false Negatives\n",
    "    for i in range(dim):\n",
    "        sum = 0\n",
    "        for j in range(dim):\n",
    "            if (i != j):\n",
    "                sum += mistake_list[j][i]\n",
    "        false[i][3] = sum\n",
    "    df = pd.DataFrame(false)\n",
    "    #, \"False Positives\", \"False Negatives\"\n",
    "    \n",
    "    df.columns = [\"Total Predictions\", \"Correct\",\"False Positives\",\"False Negatives\"]\n",
    "    df.insert(0, \"Emotion\", np.unique(emotion_list))\n",
    "    (_, counts) = np.unique(emotion_list,return_counts=True)\n",
    "   # for i in range(counts.size):\n",
    "   #     counts[i] = counts[i] / 60\n",
    "   # df.insert(1,\"Amount of Data / 10\",counts)\n",
    "    print(df.to_latex())\n",
    "    df.plot.bar(x='Emotion',figsize=(3,2),colormap=cm.get_cmap('summer'))\n",
    "    #plot = sns.barplot(x=\"Emotion\",y=Correct, data=df)\n",
    "\n",
    "def make_comparison():\n",
    "    print(\"hello\")\n",
    "    \n",
    "    (sentence_list, emotion_list) = load_data()\n",
    "\n",
    "    ### Comment the Below line for all 18 emotions. This sorts into \"positive\" and \"negative\"\n",
    "    sentence_list, emotion_list = sort_to_2_emotions(sentence_list, emotion_list)\n",
    "    print(emotion_list[0])\n",
    "    (tokenized_data,token) = tokenize(sentence_list,return_tokenizer=True)\n",
    "\n",
    "    model = train(tokenized_data, emotion_list)\n",
    "\n",
    "    #,'McDonald\\'s\n",
    "    #index = ['Chick-fil-A',\"Panera\",'McDonald\\'s','Chipotle','Pal\\'s']\n",
    "    index = ['Chick-fil-A',\"Panera\",'Chipotle','Pal\\'s','Taco Bell', 'Comcast', 'McDonalds\\'s',]\n",
    "    columns= ['Percent Positive']\n",
    "    \n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    filepaths = ['tweets_unlabeled/out2.csv',\n",
    "                 'tweets_unlabeled/panera.csv',\n",
    "                 'tweets_unlabeled/chip.csv',\n",
    "                 'tweets_unlabeled/pals.csv',\n",
    "                 'tweets_unlabeled/tacobell.csv',\n",
    "                 'tweets_unlabeled/comxfc.csv',\n",
    "                 'tweets_unlabeled/mcdn.csv',\n",
    "                ]\n",
    "    colors = [\n",
    "        'r',\n",
    "        'olivedrab',\n",
    "        'firebrick',\n",
    "        'turquoise',\n",
    "        'darkviolet',\n",
    "        'brown',\n",
    "        'gold',\n",
    "    ]\n",
    "    for (i,j,c) in zip(filepaths,index,colors):\n",
    "        tweets = load_unlabeled_tweets(i)\n",
    "       # for k in tweets:\n",
    "\n",
    "        #    print(k)\n",
    "       #     X = token.texts_to_sequences([k])\n",
    "\n",
    "        \n",
    "      #  print(tweets)\n",
    "        X = token.texts_to_sequences(tweets)\n",
    "        X = pad_sequences(X)\n",
    "        \n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        positiveonly = []\n",
    "        for i in predictions:\n",
    "            positiveonly.append(i[1])\n",
    "            \n",
    "        pos = 0\n",
    "        for p in positiveonly:\n",
    "            if p > .45:\n",
    "                pos += 1\n",
    "        print(float(pos)/len(positiveonly), \" of tweets at \", j, \" are above 53\\% postive\")\n",
    "\n",
    "        pos_df = pd.DataFrame(positiveonly)\n",
    "        pos_df.to_csv(\"stats/\"+j + \"_scores.csv\",header=None)\n",
    "        \n",
    "        loc, scale = expon.fit(positiveonly, loc=0)\n",
    "        \n",
    "        style.use('ggplot')  \n",
    "        plt.figure(figsize=(12, 9))  \n",
    "        plt.title('Sentiment Distribution of '+ j + ' Twitter Comments',fontsize=20)\n",
    "        plt.xlabel('Positive Sentiment Score',fontsize=16)\n",
    "        plt.ylabel('Percent of Scored Comments',fontsize=16)\n",
    "        plt.axis([0.0, 1.0, 0.0, 7.0])\n",
    "\n",
    "        plt.hist(positiveonly, density=True, bins=40 ,color=c)\n",
    "\n",
    "        plt.savefig('tweet_histograms/' + j + '.eps', format='eps', dpi=1200)\n",
    "        plt.savefig('tweet_histograms/' + j + '.png', format='png', dpi=1200)\n",
    "        tikz_save('tweet_histograms/' + j + '.tex',figureheight='4cm', figurewidth='6cm')\n",
    "        plt.show()\n",
    "    \n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "def cross_test():\n",
    "    \n",
    "    (sentence_list, emotion_list) = load_data()\n",
    "\n",
    "\n",
    "    ### Comment the Below line for all 18 emotions. This sorts into \"positive\" and \"negative\"\n",
    "    sentence_list, emotion_list = sort_to_2_emotions(sentence_list, emotion_list)\n",
    "\n",
    "    (tokenized_data,token) = tokenize(sentence_list,return_tokenizer=True)\n",
    "\n",
    "    (model, X_test) = train(tokenized_data, emotion_list)\n",
    "    \n",
    "    (sentence_list_self, emotion_list_self) = load_data()\n",
    "    \n",
    "    X = token.texts_to_sequences(sentence_list_self)\n",
    "    X = pad_sequences(X)\n",
    "    \n",
    "    (score,acc) = model.evaluate(x, emotion_list_self)\n",
    "    \n",
    "    print('acc')\n",
    "\n",
    "    \n",
    "\n",
    "#################################\n",
    "#############MAIN################\n",
    "#################################\n",
    "#   jupyter notebook is weird   #\n",
    "\n",
    "\n",
    "def test():\n",
    "    (sentence_list, emotion_list) = load_data()\n",
    "\n",
    "\n",
    "    ### Comment the Below line for all 18 emotions. This sorts into \"positive\" and \"negative\"\n",
    "    sentence_list, emotion_list = sort_to_2_emotions(sentence_list, emotion_list)\n",
    "\n",
    "    tokenized_data = tokenize(sentence_list)\n",
    "\n",
    "    (model, X_tesmt) = train(tokenized_data, emotion_list)\n",
    "\n",
    "    #test(model, X_test, tokenized_data, sentence_list, emotion_list)\n",
    "\n",
    "    mistake_list = count_errors(model, tokenized_data, sentence_list, emotion_list,X_test)\n",
    "\n",
    "    graph_errors(mistake_list, emotion_list)\n",
    "\n",
    "df = make_comparison()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(20,4))\n",
    "#sns.barplot(x = ProjectData['emotion'].unique(), y=ProjectData['emotion'].value_counts())\n",
    "#plt.show()\n",
    "from matplotlib import cm\n",
    "#graph_errors(mistake_list, emotion_list)\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
